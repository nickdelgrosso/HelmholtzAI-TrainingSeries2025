{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Testing with Pytest\n",
    "\n",
    "Manual testing is an integral part of software development. However, it does not scale well with the size of a project. The more your codebase grows the more time you have to spend testing. What's more, as things become more complex it becomes difficult to understanded all the interactions between different part of your code and it is easy to break one part of your program by modifying another one.\n",
    "This is where automated testing are invaluable. By building a suite of tests you can constantly verify that your code works as expected and that new features are being integrated properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psychopy pytest ipytest pytest-cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from psychopy.sound import Sound\n",
    "from psychopy.visual import Window\n",
    "from psychopy.event import waitKeys\n",
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "Sound(stereo=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Writing Assert Statements\n",
    "\n",
    "A key component of every test is the `assert` statement. It tests that result of some part of the code is what you would expect. `assert` is really simple - it raises an `AssertionError` if whateber you are asserting evaluates to `False` and does nothing otherwise.\n",
    "Generally, more asserts are better but often it is a good idea to focus your test on interesting edge cases, like: what happens when you pass in an empty list to a function that accepts lists? What if a time parameter is zero or negative? And so on ...\n",
    "\n",
    "### Reference Table\n",
    "|Code | Duration |\n",
    "| ---| ---|\n",
    "|`assert a == b` | Raise an `AssertionError` if `a` is NOT equal to `b`, otherwise do nothing |\n",
    "|`assert isinstance(a, int)` | Raise an `AssertionError` if `a` is not an integer, otherwise do nothing |\n",
    "|`assert a > b` | Raise an `AssertionError` if `a` is NOT greater than `b`, otherwise do nothing |\n",
    "|`tone = Sound(value=350, secs=0.5)` | Create a `tone` at `350`Hz with a duration of `0.5` seconds |\n",
    "|`key = waitKeys(keyList=\"space\", timeStamped=True)` | Wait until the `\"space\"` key was pressed and return a list of lists with [[name, time]].|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: `assert` that the value `x` is positive and and `print(\"Success\"!)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.random();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: `assert` that the absolute difference between x and y is smaller than 1 and and `print(\"Success\"!)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.random()\n",
    "y = random.random();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: `assert` that all elements in x are strings and and `print(\"Success\"!)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\"1\", \"2\", \"3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Write two assert statements that check that:\n",
    "- the value returned by `subtract(3, 5)` is `-2`\n",
    "- the value returned by `subtract(10, 7)` is `3`\n",
    "\n",
    "and `print(\"Success\"!)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract(a,b):\n",
    "    return a-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Write an `assert` statement that checks that:\n",
    "- the the length of the list returned by `concatenate_lists(a,b)` is equal to `len(a) + len(b)` \n",
    "\n",
    "and `print(\"Success\"!)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_lists(a,b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Write two `assert` statement that check that:\n",
    "- the tone returned by `make_tone(freq=800, dur=0.5)` has the attributes `tone.sound=800` and `tone.secs=0.5`\n",
    "- the tone returned by `make_tone(freq=1200, dur=0.3)` has the attributes `tone.sound=1200` and `tone.secs=0.3`\n",
    "\n",
    "\n",
    "and `print(\"Success\"!)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tone(freq, dur):\n",
    "    return Sound(value=freq, secs=dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Write four `assert` statement that check that:\n",
    "- The length of the list returned by `wait_keys([\"space\"])` is 1\n",
    "- The first element of the list returned by `wait_keys([\"space\"])` is `\"space\"`\n",
    "- The length of the list returned by `wait_keys([\"space\"], True)` is 2\n",
    "- The second element of the list returned by `wait_keys([\"space\"], True)` is a `float`\n",
    "\n",
    "and `print(\"Success\"!)`\n",
    "\n",
    "Hint: you have to wrap you tests inside a Window context manager, like this: <br>\n",
    "`with Window() as win:` \n",
    "<br> &nbsp;&nbsp;&nbsp;&nbsp; `assert ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_keys(keys=None, timed=False):\n",
    "    keys = waitKeys(keyList=keys, timeStamped=timed)\n",
    "    if timed:\n",
    "        return keys[0]\n",
    "    else:\n",
    "        return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing Test Functions\n",
    "\n",
    "Usually `assert` statments are not used on their own but wrapped inside test functions. Organizing your tests into functions provides the same advantages as organizing your code in functions: it makes them more readable, maintainable and robust. What's more, it allow tools such as Pytest to do the testing automatically and report to you any failed tests. In this section we are going to use Ipytest which is a convenient tool to use Pytest inside a Jupyter notebook: simply include an %%ipytest tag at the top of a cell and, when you execute it, Pytest will run every function in the cell that starts with `test_`. The rules for writing good test functions are the same as the rules for writing good function: each test should have a clear purpose an a name that reflects this purpose like `test_file_was_written()`.\n",
    "\n",
    "|Code | Description |\n",
    "| --- | ---         |\n",
    "|`%%ipytest` | Use Pytest to execute every function in a cell that starts with `test_` |\n",
    "| `with pytest.raises(ValueError):` <br> &nbsp;&nbsp;&nbsp;&nbsp; `divide(1, \"hi)` | Assert that `divide(1,\"hi\")` raises a `ValueError`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test the `trial_sequence()` function defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_sequence(conditions: list, n_reps: int, shuffle:bool = True):\n",
    "    trials = conditions * n_reps\n",
    "    if shuffle:\n",
    "        random.shuffle(trials)\n",
    "    return trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Write a test function `test_trial_sequence_is_shuffled()` that tests that the returned list is shuffled when `shuffle=True`. Add the `%%ipytest` tag to the top of the cell and run it to execute that cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def test_trial_sequence_is_shuffled():\n",
    "    trials1 = trial_sequence(conditions=[1,2,3], n_reps=1000)\n",
    "    trials2 = trial_sequence(conditions=[1,2,3], n_reps=1000)\n",
    "    assert trials1 != trials2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Write a test function `test_trial_sequence_is_shuffled()` that tests that the returned list is ordered when `shuffle=True`. Add the `%%ipytest` tag to the top of the cell and run it to execute that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Write a test function that tests the list returned by `test_trial_sequence_is_shuffled()` has the correct `len()` using three different `assert` conditions . Add the `%%ipytest` tag to the top of the cell and run it to execute that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We are going to test the `load_config` function defined in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(fpath:str):\n",
    "    with open(fpath) as f:\n",
    "        config = json.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Write a test function that tests that the dictionary returned by `load_config()` contains the keys `\"conditions\"` and `\"n_trials\"`.\n",
    "\n",
    " (Hint: `\"a\" in config` is `True` if `config` contains a key named `\"a\"`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Write a test function that tests the value stored under the key \"conditions\" in the dictionary returned by `load_config()` is a list of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Write a test function that tests that uses `pytest.raises` to test that `load_config` raises a `FileNotFoundError` when trying to load a file that does not exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Runnig Pytest from the Terminal\n",
    "\n",
    "Now that we understand how Pytest works we can start to develop our test module. The tests work exactly the same as in a notebook but you put them in a separate file. Usually this file is named like the file it tests with the \"test_\" prefix (e.g. `test_my_module.py` would contain the tests for `my_module`.py). In the test module, you can import the functions you want to test and then test them.\n",
    "Organizing your test like this has the advantage that you can run all of them by simply calling pytest a single time.\n",
    "Per default, Pytest will open every Python file that starts with `test_` and, inside that file, run every function that starts with `test_`.\n",
    "\n",
    "This folder contains a function called `sequencegen.py` by that contains 3 functions:\n",
    "- `has_repetitions()` which checks if a trial seuquence has repetitions of the same element within the given `min_dist` and returns `True` or `False`\n",
    "- `make_sequene()` that creates a sequence from a list of `conditions` with a given number of `n_trials` and shuffles that list until `has_repetitions()` returns `False`\n",
    "- `write_sequence()` which writes the trials sequence to a file\n",
    "\n",
    "Let's test these functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Exercise**: In this folder, create a new script called `test_sequencegen.py` where you `import` the three functions from `sequencegen.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: In this folder create a new function called `test_has_repetitions()` that tests that:\n",
    "- Calling `test_has_repetitions()` on a list that has a repetition returns `True`\n",
    "- Calling `test_has_repetitions()` on a list that has a repetition returns `False`\n",
    "\n",
    "Then execute the cell below to run Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: add another function called `test_has_repetitions_respects_min_dist` to test that:\n",
    "- Calling `test_has_repetitions()` on a list that has a repetition further away that `min_dist` returns `False`\n",
    "- Calling `test_has_repetitions()` on a list that has a repetition within `min_dist` returns `True`\n",
    "\n",
    "Then execute the cell below to run Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**: Write a function called `test_sequence_has_correct_len()` that tests that the sequence returned by `make_sequence` has desired length given by `n_trials`.\n",
    "\n",
    "Then execute the cell below to run Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**: Write a function called `test_max_iterations()`, that uses `pytest.raises` to test that `make_sequence()` raises a `StopIteration` if you try to generate an impossible sequence (e.g. a sequence with a `min_dist` that is too large).\n",
    "\n",
    "Then execute the cell below to run Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice feature of pytest is that it can determine our test coverage which tells us the percentage of code that is covered by our tests.\n",
    "A test coverage below 100% means that there are certain parts of your code that are never executed.\n",
    "This can help us to identify any blind spots in our code. Just run the cell below to let Pytest mesure the test coverage for `sequencegen.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytet --cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameterizing Tests\n",
    "\n",
    "Writing assert statements to tests all the different combinations of parameters that may be important for you program can be laborious. This is where parameterization is extremely useful. It provides us with an esy interface to run a large number of test. For example, we could test a function like `add` with a large number of values by parameterizing it with the `range()` function.\n",
    "Pytets will take all those values and run our test function with every single one.\n",
    "For this to work, the name in parameterize `\"a\"` must have the same name as the parameter of the test function `a`.\n",
    "\n",
    "```python\n",
    "@pytest.mark.parametrize(\"a\", range(1000))\n",
    "def test_add(a)\n",
    "    add(a+3) == a+3\n",
    "```\n",
    "\n",
    "We can also parameterize multiple parameters. In this case, we pass in both variable names followed by a list of tuples, each of which contains the parameters for one run of the test. In the example below, `a` will take on the values 1, 3, and 4 and `b` will take on the values `2`, `4` and `6`.\n",
    "\n",
    "```python\n",
    "@pytest.mark.parametrize(\"a, b\", [(1,2), (3,4), (5,6)])\n",
    "def test_add(a, b):\n",
    "    assert add(a+b) == a+b \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Parameterize the function `test_sequence_has_correct_len()` so that it tests that the sequence returned by `make_sequence` has desired length for  different values of `n_trials`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Parameterize the function `test_max_iteration()` so that it tests that it raises a `StopIteration` if you try to generate a sequence with three different impossible combionations of `conditions` and `n_trials`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psychopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
