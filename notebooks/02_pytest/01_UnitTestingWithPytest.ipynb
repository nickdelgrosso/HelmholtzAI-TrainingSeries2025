{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pytest ipytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de130b",
   "metadata": {},
   "source": [
    "# Pre-Workshop Exercises: Unit Testing with `pytest`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f4cdf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3a3ea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What is Automated Testing?\n",
    "\n",
    "Anything that your code can do, you can check.  If you find yourself manually checking for something, putting that check in an automated test helps you continue doing it as you develop your project further, automatically.\n",
    "\n",
    "[`pytest`](https://docs.pytest.org/en/stable/) contains a test runner that looks for automated tests; one way it finds them is by looking for function names that start with the word `test_`, in file names that start with the word `test_`.  It runs each function it finds, and marks down whether running it:\n",
    "  - **Passed**: The function ran with no errors\n",
    "  - **Failed**: The function ran with an `AssertionError`\n",
    "  - **Errored**: The function ran with any other error type.\n",
    "\n",
    "While [PyTest] is far and away the most popular testing framework in Python, there are other alternatives, including:\n",
    "\n",
    "| Library | When to Choose it over Pytest |\n",
    "| :-- | :-- |\n",
    "| [`unittest`](https://docs.python.org/3/library/unittest.html) | When you want something built-in to Python. |\n",
    "| [`doctest`](https://docs.python.org/3/library/doctest.html) | When you want automated test code in your function ducumentation | \n",
    "| [`behave`](https://behave.readthedocs.io/en/latest/) | When you want automated test code to be readable and writable by non-coding teammates |\n",
    "| [A PyTest Plugin](https://docs.pytest.org/en/stable/reference/plugin_list.html) | When you want to add features to your tests, make special test types easier to write, or work with tricky-to-test frameworks. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cbe9dd",
   "metadata": {},
   "source": [
    "\n",
    "### Why Write Automated Tests?\n",
    "\n",
    "While software developers often discuss automated testing in terms of quality control for gaining others' trust in our code in large projects, in practice, automated testing tools are used for a wide variety of development tasks that help speed up the development process.  Here are just a few examples, to show how pervasive automated testing is:\n",
    "\n",
    "  1. **Checklist Automation**: Do you find yourself rerunning your code a lot to confirm that it still works?  This can slow down our workflow and hurt our creative flow, if it takes too long.  For each check you need to do, just write an automated test and free up our brain space!\n",
    "\n",
    "  1. **Colleague Onboarding**: Is everything ready for your colleagues to make contribution to your code?  If they can run some automated tests, then you can be confident that things are installed and ready on their machines.\n",
    "\n",
    "  1. **Troubleshooting Time Reduction**: Each time you run your code, does it take you a few minutes to work out  where the problem is?  Unit tests turn those minutes into seconds, making it easier to quickly pin down why our code isn't working. \n",
    " \n",
    "  1. **Design Tools for Tricky Algorithms**: Do you feel like a  particular function feels more like a brain teaser than usual?  Write a few tests as you work on it, to free up some brain space that's focused on checking the code. \n",
    "\n",
    "  1. **Code Structuring Guidance**: Wondering if your code is still modular?  Writing tests is a great software architecture check--unit-testable code is modular code!\n",
    "\n",
    "  1. **UX Guidance**: Wondering if your functions are intuitive to use for others? A great check is to write unit tests, and see if the test code is complicated.  Simple tests mean intuitive interfaces!\n",
    "\n",
    "  1. **Getting Started Help**: Not even sure how to get started with a project, or what you really should do?  Write a test on a program you would *like* to write!  The test will fail, of course (because the code isn't written yet), but you'll then have a clear idea of what needs done, and in what order.\n",
    "\n",
    "  1. **Bug Fixing**: A user is reporting a bug in your code?  Write an automated test to recreate the bug, so that the test fails when the bug is present.  Then, fix the code!  \n",
    "\n",
    "  1. **Code Review Simplification**: Want to speed up code review when accepting contributions?  Require tests on new code! If you're happy with the tests, and they pass, then the code is likely already good to go.\n",
    "\n",
    "  1. **Mentoring Aid**: Have a junior who wants to contribute, but isn't sure how?  Sit together with them and write some tests that they should get to pass.  That way they have feedback on their progress to their goal, and you get code that works!\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15bed2",
   "metadata": {},
   "source": [
    "### Unit Tests vs \"Other\" Tests\n",
    "\n",
    "We talk a lot about unit tests, because they are easiest to make, and in a big project they make up the vast majority of the automated tests, but there are a lot of different types of tests--which you choose just depends on your goals for that test.  Here are a few other options out there:\n",
    "\n",
    "| Test Type | When you want to... | Example |  \n",
    "| :-- | :-- | :-- |\n",
    "| **Unit Test** | Check that a function or method works. | Check: Calling `predict([1, 2])` returns a transformed array `[3, 4]`. |\n",
    "| **Property Test** | A type of unit test, it checks that all calls to a function or method result something with a desired property. | Check: Calling `predict()` always returns a 1D array of floats. |\n",
    "| **Integration Test** | Check that a function, method, or class calls other functions or methods the way you expected. | Check: Calling `predict()` calls the OpenAI API with certain parameters. |\n",
    "| **System Test** | Check that the whole project works on a high level.  | Check: when I run my pipeline on my data, I get the figures I want. |\n",
    "| **Smoke Test** | Checks that nothing is crashing. | Check: When I run my script, it doesn't error out. |\n",
    "| **Behavior Test** | Checks that the program works along the user's expectations | Check: when I press the `fit` button, I see a model fit on-screen. |\n",
    "| **Snapshot Test** | Checks that the program still does the same thing it did yesterday.  | Check: my pipeline still produces the same figure it did last time I ran the test. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f069c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Writing automated tests is work--it doesn't come for free.  Through these exercises, we'll get familiar with the basics of the `pytest` framework and a few useful supplmentary libraries, writing unit tests in a concise manner, so we can spend less time writing boilerplate test code and more time building our projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91299c98",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "To make it easy to write and run automated tests in a notebook, we're using the [`ipytest`](https://pypi.org/project/ipytest/) package.  Run the code below to get it set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to install the packages used in these exercises\n",
    "# %pip install pytest ipytest hypothesis numpy pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830adf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'addopts': ['-q',\n",
       "  '-W',\n",
       "  'ignore:Module already imported so cannot be rewritten:pytest.PytestAssertRewriteWarning'],\n",
       " 'clean': '[Tt]est*',\n",
       " 'coverage': False,\n",
       " 'defopts': 'auto',\n",
       " 'display_columns': 100,\n",
       " 'magics': True,\n",
       " 'raise_on_error': False,\n",
       " 'rewrite_asserts': False,\n",
       " 'run_in_thread': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "# Makes the \"%%ipytest cell magick work, set options\"\n",
    "# or just call `ipytest.autoconfig()` without options to make it all work easily.\n",
    "ipytest.config(\n",
    "    magics=True, \n",
    "    defopts=\"auto\", \n",
    "    addopts=[\n",
    "        \"-q\",  # quiet output\n",
    "        \"-W\", \"ignore:Module already imported so cannot be rewritten:pytest.PytestAssertRewriteWarning\",\n",
    "    ],\n",
    "    coverage=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54155498",
   "metadata": {},
   "source": [
    "### 1. Checking Test Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e73c43",
   "metadata": {},
   "source": [
    "\n",
    "Automated Tests three main parts:  a **Test Runner**, **Test Functions** and **Test Assertions**.\n",
    "\n",
    "| Term | Code | Description |\n",
    "| :-- | :-- | :-- |\n",
    "| **Test Runner** | `%%ipytest` | The program that finds, runs, and records tests and their results. |\n",
    "| **Test Function** | `def test_xxx():` | The function run by the test runner. Must start with `test_`, followed by a description of what is checked.  |  \n",
    "| **Test Assertion** | `assert x` | The check itself.  If all checks in a test function pass without error, then the test is considered to have passed. |\n",
    "\n",
    "\n",
    "Let's start with pre-written tests, fixing small parts of the code to see how the test runner gives feedback on what it finds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cd551",
   "metadata": {},
   "source": [
    "\n",
    "**Task**: One of the tests below has an error in it, and so the test is failing. Use the output from pytest to find the failing test and fix it so all tests pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068eb224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                           [100%]\u001b[0m\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m________________________________________ test_sum_2_3_is_5 ________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_sum_2_3_is_5\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96msum\u001b[39;49;00m([\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m]) == \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\delgr\\AppData\\Local\\Temp\\ipykernel_40504\\2304590615.py\u001b[0m:6: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info =====================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_a350e818289a44d1b28e7c328c41e720.py::\u001b[1mtest_sum_2_3_is_5\u001b[0m - AssertionError\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.60s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_sum_1_2_is_3():\n",
    "    assert sum([1, 2]) == 3\n",
    "\n",
    "\n",
    "def test_sum_2_3_is_5():\n",
    "    assert sum([1, 2]) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65803f0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task**: Below are three unit tests that check for three different types of things: a value, a type, and an error.  Edit the code so all tests do their intended checks successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36776f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                          [100%]\u001b[0m\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m________________________________________ test_sum_3_4_is_7 ________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_sum_3_4_is_7\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Hint: assert sum([..., ...]) == ...\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\delgr\\AppData\\Local\\Temp\\ipykernel_40504\\998771727.py\u001b[0m:3: NotImplementedError\n",
      "\u001b[31m\u001b[1m___________________________________ test_sum_of_ints_is_an_int ____________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_sum_of_ints_is_an_int\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Hint: assert isinstance(sum([...]), int)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\delgr\\AppData\\Local\\Temp\\ipykernel_40504\\998771727.py\u001b[0m:7: NotImplementedError\n",
      "\u001b[31m\u001b[1m______________________________ test_sum_strings_a_b_raises_typeerror ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_sum_strings_a_b_raises_typeerror\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\delgr\\AppData\\Local\\Temp\\ipykernel_40504\\998771727.py\u001b[0m:10: NotImplementedError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info =====================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_a350e818289a44d1b28e7c328c41e720.py::\u001b[1mtest_sum_3_4_is_7\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m t_a350e818289a44d1b28e7c328c41e720.py::\u001b[1mtest_sum_of_ints_is_an_int\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m t_a350e818289a44d1b28e7c328c41e720.py::\u001b[1mtest_sum_strings_a_b_raises_typeerror\u001b[0m - NotImplementedError\n",
      "\u001b[31m\u001b[31m\u001b[1m3 failed\u001b[0m\u001b[31m in 0.05s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_sum_3_4_is_7():\n",
    "    # Hint: assert sum([..., ...]) == ...\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def test_sum_of_ints_is_an_int():\n",
    "    # Hint: assert isinstance(sum([...]), int)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def test_sum_strings_a_b_raises_typeerror():\n",
    "    raise NotImplementedError()\n",
    "    with pytest.raises(TypeError):\n",
    "        ... # Put code that should result in an error here.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0041a",
   "metadata": {},
   "source": [
    "### 2. Checking Equality of Numpy Arrays with `numpy.testing` and Pandas DataFrames with `pandas.testing`\n",
    "\n",
    "Because we do a lot of data science work, we often check that arrays and dataframes are what we expected.  To make this check easier, many packages include a `testing` subpackage with special `assert_()` functions used to simplify writing tests on  their data structures.  Not only do they make the code easier to write, they also usually give quite descriptive error messages when the tests fail, making troubleshooting simpler.  Let's try it out with `numpy` and `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c0375",
   "metadata": {},
   "source": [
    "**Task**: Write a unit test to check that the computed numpy array is the expected one.  When the test fails, use the error messages to fix the test so that it passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f93ec10b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\nMismatched elements: 1 / 3 (33.3%)\nMax absolute difference among violations: 1\nMax relative difference among violations: 0.11111111\n ACTUAL: array([5, 7, 8])\n DESIRED: array([5, 7, 9])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m expected = np.array([\u001b[32m5\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m8\u001b[39m])\n\u001b[32m      8\u001b[39m observed = a + b\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mnpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_array_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\delgr\\Projects\\hhai-repo\\.pixi\\envs\\default\\Lib\\site-packages\\numpy\\testing\\_private\\utils.py:926\u001b[39m, in \u001b[36massert_array_compare\u001b[39m\u001b[34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\u001b[39m\n\u001b[32m    921\u001b[39m         err_msg += \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m + \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m.join(remarks)\n\u001b[32m    922\u001b[39m         msg = build_err_msg([ox, oy], err_msg,\n\u001b[32m    923\u001b[39m                             verbose=verbose, header=header,\n\u001b[32m    924\u001b[39m                             names=names,\n\u001b[32m    925\u001b[39m                             precision=precision)\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    928\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtraceback\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: \nArrays are not equal\n\nMismatched elements: 1 / 3 (33.3%)\nMax absolute difference among violations: 1\nMax relative difference among violations: 0.11111111\n ACTUAL: array([5, 7, 8])\n DESIRED: array([5, 7, 9])"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33m\u001b[33mno tests ran\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "# npt.assert_array_equal()\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "expected = np.array([5, 7, 8])\n",
    "observed = a + b\n",
    "npt.assert_array_equal(expected, observed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f0d2e",
   "metadata": {},
   "source": [
    "**Task**: Write a unit test to check whether the two methods below produce the same dataframes.  When the test fails, use the error messages to fix the test so that it passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "176c1cb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "DataFrame.columns are different\n\nDataFrame.columns values are different (100.0 %)\n[left]:  Index(['a', 'b'], dtype='object')\n[right]: Index(['b', 'a'], dtype='object')\nAt positional index 0, first diff: a != b",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m df2[\u001b[33m'\u001b[39m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m] = [\u001b[32m10\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m12\u001b[39m]\n\u001b[32m     11\u001b[39m df2[\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m] = [\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mpdt\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_frame_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/testing.pyx:55\u001b[39m, in \u001b[36mpandas._libs.testing.assert_almost_equal\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/testing.pyx:173\u001b[39m, in \u001b[36mpandas._libs.testing.assert_almost_equal\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\delgr\\Projects\\hhai-repo\\.pixi\\envs\\default\\Lib\\site-packages\\pandas\\_testing\\asserters.py:620\u001b[39m, in \u001b[36mraise_assert_detail\u001b[39m\u001b[34m(obj, message, left, right, diff, first_diff, index_values)\u001b[39m\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_diff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    618\u001b[39m     msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfirst_diff\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n",
      "\u001b[31mAssertionError\u001b[39m: DataFrame.columns are different\n\nDataFrame.columns values are different (100.0 %)\n[left]:  Index(['a', 'b'], dtype='object')\n[right]: Index(['b', 'a'], dtype='object')\nAt positional index 0, first diff: a != b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33m\u001b[33mno tests ran\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "import pandas as pd\n",
    "import pandas.testing as pdt\n",
    "# pdt.assert_frame_equal()\n",
    "\n",
    "# Method one: from dictionary\n",
    "df1 = pd.DataFrame({'a': [1, 2, 3], 'b': [10, 11, 12]})\n",
    "\n",
    "# Method two: Stepwise DataFrame Mutation\n",
    "df2 = pd.DataFrame()\n",
    "df2['b'] = [10, 11, 12]\n",
    "df2['a'] = [1, 3, 3]\n",
    "\n",
    "pdt.assert_frame_equal(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f4a27",
   "metadata": {},
   "source": [
    "### 3. Test Parameterization: Check More Cases with Less Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c5282",
   "metadata": {},
   "source": [
    "\n",
    "It's valuable to check many different sets of inputs and confirm that the outputs are all correct; strange little bugs appear in many functions when certain values weren't what we expected.  But writing a function for every single set of inputs we want to check for is needlessly verbose.  `Parametrizing` tests functions makes that code more condensed, and PyTest provides a decorator for doing this `@pytest.mark.parametrize()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc683c",
   "metadata": {},
   "source": [
    "\n",
    "**Task**: The code below uses PyTest's parametrization feature.  Without writing a new function, use that feature to add two more checks (a.k.a. \"test cases\") to the tests below, so that a total of 4 tests run:\n",
    "  - `3 + 7 = 10`\n",
    "  - `-2 + 3 = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1362f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "cases = [\n",
    "    [[1, 2], 3],\n",
    "    [[2, 3], 5],\n",
    "]\n",
    "@pytest.mark.parametrize('inputs,output', cases)\n",
    "def test_sum_of_integers(inputs, output):\n",
    "    assert sum(inputs) == output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d44c6b",
   "metadata": {},
   "source": [
    "**Task**: Rewrite the three test functions below into a single test function, using `parametrize` to continue checking each case individually. Note that pytest includes an `approx()` function for helping check floats, since there are often little rounding errors with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_5p2_minus_2p1_is_3p1():\n",
    "    assert 5.2 - 2.1 == pytest.approx(3.1)\n",
    "\n",
    "def test_6p5_minus_1p7_is_4p8():\n",
    "    assert 6.5 - 1.7 == pytest.approx(4.8)\n",
    "\n",
    "def test_0p3_minus_0p2_is_0p1():\n",
    "    assert 0.3 - 0.2 == pytest.approx(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53223af0",
   "metadata": {},
   "source": [
    "### 4. Property Testing with `hypothesis`\n",
    "\n",
    "There are also cases where you want to check a bunch of inputs to make sure that the code works as correctly, but:\n",
    "  -  you don't know exactly which inputs are the best to check,\n",
    "  -  and you aren't sure exactly how to calculate the expected result,\n",
    "  -  but you know what aspect of the result you want to check (i.e. \"property\" you want the result to have).\n",
    "\n",
    "This is called \"**Property Testing**\", and the `hypothesis` library helps with that.  Just describe the inputs that should go in, and write your test, and it will check your code with a wide range of inputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e5846",
   "metadata": {},
   "source": [
    "**Task**: The test function below isn't checking what it means to be (as described by the function name), and so Hypothesis keeps finding sets of inputs that make the test fail.  Fix the inputs and the test function body, so the test is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest \n",
    "\n",
    "from hypothesis import given\n",
    "from hypothesis import strategies as st\n",
    "# For the curious, a full list of \"strategy\" functions (how hypothesis generates inputs): \n",
    "# https://hypothesis.readthedocs.io/en/latest/reference/strategies.html\n",
    "\n",
    "\n",
    "@given(\n",
    "    st.lists(st.integers(min_value=-10), min_size=0),\n",
    ")\n",
    "def test_sum_of_positive_integers_always_a_positive_integer(inputs):\n",
    "    assert sum(inputs) > 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb171e",
   "metadata": {},
   "source": [
    "**Task**: Have hypothesis generate `float` values in order to test the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d2e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest \n",
    "\n",
    "from hypothesis import given\n",
    "from hypothesis import strategies as st\n",
    "\n",
    "@given(\n",
    "    # Put a strategy here for the first float\n",
    "    # Put a strategy here for the second float\n",
    ")\n",
    "def test_sum_of_two_floats_is_always_equivalent_to_using_plus_operator(first, second):\n",
    "    assert sum([first, second]) == pytest.approx(first + second)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
