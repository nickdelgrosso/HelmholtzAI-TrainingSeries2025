{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pytest ipytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91299c98",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830adf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "# Makes the \"%%ipytest cell magick work, set options\"\n",
    "# or just call `ipytest.autoconfig()` without options to make it all work easily.\n",
    "ipytest.config(\n",
    "    magics=True, \n",
    "    defopts=\"auto\", \n",
    "    addopts=[\n",
    "        \"-q\",  # quiet output\n",
    "        \"-W\", \"ignore:Module already imported so cannot be rewritten:pytest.PytestAssertRewriteWarning\",\n",
    "    ],\n",
    "    coverage=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54155498",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533bd4ef",
   "metadata": {},
   "source": [
    "### 1. Unit Testing with `pytest`\n",
    "\n",
    "Anything that your code can do, you can check.  If you find yourself manually checking for something, putting that check in an automated test helps you continue doing it as you develop your project further, automatically.\n",
    "\n",
    "`pytest` contains a test runner that looks for automated tests; one way it finds them is by looking for function names that start with the word `test_`, in file names that start with the word `test_`.  It runs each function it finds, and marks down whether running it:\n",
    "  - **Passed**: The function ran with no errors\n",
    "  - **Failed**: The function ran with an `AssertionError`\n",
    "  - **Errored**: The function ran with any other error type.\n",
    "\n",
    "Let's try it out!  Here we'll be writing **\"Unit Tests\"**, which check that some function or class is working properly (e.g. a function returns the expected thing when called).  Since coding projects tend to be made up of lots of custom functions, it's good to know that the individual custom functions each work the way they should."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cd551",
   "metadata": {},
   "source": [
    "\n",
    "**Task**: One of the tests below has an error in it, and so the test is failing. Use the output from pytest to find the failing test and fix it so all tests pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068eb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_sum_1_2_is_3():\n",
    "    assert sum([1, 2]) == 3\n",
    "\n",
    "\n",
    "def test_sum_2_3_is_5():\n",
    "    assert sum([1, 2]) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65803f0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task**: Below are three unit tests that check for three different types of things: a value, a type, and an error.  Edit the code so all tests do their intended checks successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36776f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_sum_3_4_is_7():\n",
    "    # Hint: assert sum([..., ...]) == ...\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def test_sum_of_ints_is_an_int():\n",
    "    # Hint: assert isinstance(sum([...]), int)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def test_sum_strings_a_b_raises_typeerror():\n",
    "    raise NotImplementedError()\n",
    "    with pytest.raises(TypeError):\n",
    "        ... # Put code that should result in an error here.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58066e",
   "metadata": {},
   "source": [
    "## Test Parameterization: Check More Cases with Less Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da04c8",
   "metadata": {},
   "source": [
    "Of course, writing a function for every single set of inputs we want to check for is needlessly verbose.  `Parametrizing` tests functions makes that code more condensed, and PyTest provides a decorator for doing this `@pytest.mark.parametrize()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b844f",
   "metadata": {},
   "source": [
    "\n",
    "**Task**: Add two more checks (a.k.a. \"test cases\") to the tests below, so that a total of 4 tests run:\n",
    "  - `3 + 7 = 10`\n",
    "  - `-2 + 3 = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a15410",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "cases = [\n",
    "    [[1, 2], 3],\n",
    "    [[2, 3], 5],\n",
    "]\n",
    "@pytest.mark.parametrize('inputs,output', cases)\n",
    "def test_sum_of_integers(inputs, output):\n",
    "    assert sum(inputs) == output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61677a94",
   "metadata": {},
   "source": [
    "**Task**: Rewrite the three test functions below into a single test function, using `parametrize` to continue checking each case individually. Note that pytest includes an `approx()` function for helping check floats, since there are often little rounding errors with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26224730",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_5p2_minus_2p1_is_3p1():\n",
    "    assert 5.2 - 2.1 == pytest.approx(3.1)\n",
    "\n",
    "def test_6p5_minus_1p7_is_4p8():\n",
    "    assert 6.5 - 1.7 == pytest.approx(4.8)\n",
    "\n",
    "def test_0p3_minus_0p2_is_0p1():\n",
    "    assert 0.3 - 0.2 == pytest.approx(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0041a",
   "metadata": {},
   "source": [
    "## Checking Equality of Numpy Arrays with `numpy.testing` and Pandas DataFrames with `pandas.testing`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c0375",
   "metadata": {},
   "source": [
    "**Task**: Write a unit test to check that the computed numpy array is the expected one.  When the test fails, use the error messages to fix the test so that it passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f93ec10b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\nMismatched elements: 1 / 3 (33.3%)\nMax absolute difference among violations: 1\nMax relative difference among violations: 0.11111111\n ACTUAL: array([5, 7, 8])\n DESIRED: array([5, 7, 9])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m expected = np.array([\u001b[32m5\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m8\u001b[39m])\n\u001b[32m      8\u001b[39m observed = a + b\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mnpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_array_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\delgr\\Projects\\hhai-repo\\.pixi\\envs\\default\\Lib\\site-packages\\numpy\\testing\\_private\\utils.py:926\u001b[39m, in \u001b[36massert_array_compare\u001b[39m\u001b[34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\u001b[39m\n\u001b[32m    921\u001b[39m         err_msg += \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m + \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m.join(remarks)\n\u001b[32m    922\u001b[39m         msg = build_err_msg([ox, oy], err_msg,\n\u001b[32m    923\u001b[39m                             verbose=verbose, header=header,\n\u001b[32m    924\u001b[39m                             names=names,\n\u001b[32m    925\u001b[39m                             precision=precision)\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    928\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtraceback\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: \nArrays are not equal\n\nMismatched elements: 1 / 3 (33.3%)\nMax absolute difference among violations: 1\nMax relative difference among violations: 0.11111111\n ACTUAL: array([5, 7, 8])\n DESIRED: array([5, 7, 9])"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33m\u001b[33mno tests ran\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "# npt.assert_array_equal()\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "expected = np.array([5, 7, 8])\n",
    "observed = a + b\n",
    "npt.assert_array_equal(expected, observed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f0d2e",
   "metadata": {},
   "source": [
    "**Task**: Write a unit test to check whether the two methods below produce the same dataframes.  When the test fails, use the error messages to fix the test so that it passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "176c1cb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "DataFrame.columns are different\n\nDataFrame.columns values are different (100.0 %)\n[left]:  Index(['a', 'b'], dtype='object')\n[right]: Index(['b', 'a'], dtype='object')\nAt positional index 0, first diff: a != b",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m df2[\u001b[33m'\u001b[39m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m] = [\u001b[32m10\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m12\u001b[39m]\n\u001b[32m     11\u001b[39m df2[\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m] = [\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mpdt\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_frame_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/testing.pyx:55\u001b[39m, in \u001b[36mpandas._libs.testing.assert_almost_equal\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/testing.pyx:173\u001b[39m, in \u001b[36mpandas._libs.testing.assert_almost_equal\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\delgr\\Projects\\hhai-repo\\.pixi\\envs\\default\\Lib\\site-packages\\pandas\\_testing\\asserters.py:620\u001b[39m, in \u001b[36mraise_assert_detail\u001b[39m\u001b[34m(obj, message, left, right, diff, first_diff, index_values)\u001b[39m\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_diff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    618\u001b[39m     msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfirst_diff\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n",
      "\u001b[31mAssertionError\u001b[39m: DataFrame.columns are different\n\nDataFrame.columns values are different (100.0 %)\n[left]:  Index(['a', 'b'], dtype='object')\n[right]: Index(['b', 'a'], dtype='object')\nAt positional index 0, first diff: a != b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33m\u001b[33mno tests ran\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "import pandas as pd\n",
    "import pandas.testing as pdt\n",
    "# pdt.assert_frame_equal()\n",
    "\n",
    "# Method one: from dictionary\n",
    "df1 = pd.DataFrame({'a': [1, 2, 3], 'b': [10, 11, 12]})\n",
    "\n",
    "# Method two: Stepwise DataFrame Mutation\n",
    "df2 = pd.DataFrame()\n",
    "df2['b'] = [10, 11, 12]\n",
    "df2['a'] = [1, 3, 3]\n",
    "\n",
    "pdt.assert_frame_equal(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53223af0",
   "metadata": {},
   "source": [
    "## Property Testing with `hypothesis`\n",
    "\n",
    "There are also cases where you want to check a bunch of inputs to make sure that the code works as correctly, but:\n",
    "  -  you don't know exactly which inputs are the best to check,\n",
    "  -  and you aren't sure exactly how to calculate the expected result,\n",
    "  -  but you know what aspect of the result you want to check (i.e. \"property\" you want the result to have).\n",
    "\n",
    "This is called \"**Property Testing**\", and the `hypothesis` library helps with that.  Just describe the inputs that should go in, and write your test, and it will check your code with a wide range of inputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e5846",
   "metadata": {},
   "source": [
    "**Task**: The test function below isn't checking what it means to be (as described by the function name), and so Hypothesis keeps finding sets of inputs that make the test fail.  Fix the inputs and the test function body, so the test is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest \n",
    "\n",
    "from hypothesis import given\n",
    "from hypothesis import strategies as st\n",
    "# For the curious, a full list of \"strategy\" functions (how hypothesis generates inputs): \n",
    "# https://hypothesis.readthedocs.io/en/latest/reference/strategies.html\n",
    "\n",
    "\n",
    "@given(\n",
    "    st.lists(st.integers(min_value=-10), min_size=0),\n",
    ")\n",
    "def test_sum_of_positive_integers_always_a_positive_integer(inputs):\n",
    "    assert sum(inputs) > 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb171e",
   "metadata": {},
   "source": [
    "**Task**: Have hypothesis generate `float` values in order to test the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d2e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest \n",
    "\n",
    "from hypothesis import given\n",
    "from hypothesis import strategies as st\n",
    "\n",
    "@given(\n",
    "    # Put a strategy here for the first float\n",
    "    # Put a strategy here for the second float\n",
    ")\n",
    "def test_sum_of_two_floats_is_always_equivalent_to_using_plus_operator(first, second):\n",
    "    assert sum([first, second]) == pytest.approx(first + second)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
